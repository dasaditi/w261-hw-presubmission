{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW3\" data-toc-modified-id=\"Assignment---HW3-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW3</a></div><div class=\"lev3 toc-item\"><a href=\"#HW3.0.\" data-toc-modified-id=\"HW3.0.-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>HW3.0.</a></div><div class=\"lev3 toc-item\"><a href=\"#Merge-Sort-Illustrated\" data-toc-modified-id=\"Merge-Sort-Illustrated-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Merge Sort Illustrated</a></div><div class=\"lev3 toc-item\"><a href=\"#HW3.1\" data-toc-modified-id=\"HW3.1-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>HW3.1</a></div><div class=\"lev3 toc-item\"><a href=\"#HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters\" data-toc-modified-id=\"HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters-114\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2-Solutions-Part-1\" data-toc-modified-id=\"3.2-Solutions-Part-1-115\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>3.2 Solutions Part 1</a></div><div class=\"lev2 toc-item\"><a href=\"#Explain\" data-toc-modified-id=\"Explain-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Explain</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2-Solutions-Part-2\" data-toc-modified-id=\"3.2-Solutions-Part-2-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>3.2 Solutions Part 2</a></div><div class=\"lev2 toc-item\"><a href=\"#3.2-Solutions-Part-3\" data-toc-modified-id=\"3.2-Solutions-Part-3-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>3.2 Solutions Part 3</a></div><div class=\"lev3 toc-item\"><a href=\"#Step-1.-Count-words-with-combiner\" data-toc-modified-id=\"Step-1.-Count-words-with-combiner-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Step 1. Count words with combiner</a></div><div class=\"lev3 toc-item\"><a href=\"#Step-2.-Calculate-Relative-Frequencies\" data-toc-modified-id=\"Step-2.-Calculate-Relative-Frequencies-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Step 2. Calculate Relative Frequencies</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.1\" data-toc-modified-id=\"3.2.1-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>3.2.1</a></div><div class=\"lev2 toc-item\"><a href=\"#3.2.1-Solution\" data-toc-modified-id=\"3.2.1-Solution-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>3.2.1 Solution</a></div><div class=\"lev4 toc-item\"><a href=\"#Step-1.-Get-counts.-This-was-done-in-above-section,-hense-reusing-the-word-count-output-from-hw3.2.\" data-toc-modified-id=\"Step-1.-Get-counts.-This-was-done-in-above-section,-hense-reusing-the-word-count-output-from-hw3.2.-1401\"><span class=\"toc-item-num\">1.4.0.1&nbsp;&nbsp;</span>Step 1. Get counts. This was done in above section, hense reusing the word count output from hw3.2.</a></div><div class=\"lev4 toc-item\"><a href=\"#Step-2.-Inspect-the-distribution\" data-toc-modified-id=\"Step-2.-Inspect-the-distribution-1402\"><span class=\"toc-item-num\">1.4.0.2&nbsp;&nbsp;</span>Step 2. Inspect the distribution</a></div><div class=\"lev4 toc-item\"><a href=\"#Step-3.-Partition-the-counts-into-2-groups-for-use-with-2-reducers\" data-toc-modified-id=\"Step-3.-Partition-the-counts-into-2-groups-for-use-with-2-reducers-1403\"><span class=\"toc-item-num\">1.4.0.3&nbsp;&nbsp;</span>Step 3. Partition the counts into 2 groups for use with 2 reducers</a></div><div class=\"lev4 toc-item\"><a href=\"#Step-4.-Sort\" data-toc-modified-id=\"Step-4.-Sort-1404\"><span class=\"toc-item-num\">1.4.0.4&nbsp;&nbsp;</span>Step 4. Sort</a></div><div class=\"lev4 toc-item\"><a href=\"#Step-6.-Display-top-50-and-bottom-10-words\" data-toc-modified-id=\"Step-6.-Display-top-50-and-bottom-10-words-1405\"><span class=\"toc-item-num\">1.4.0.5&nbsp;&nbsp;</span>Step 6. Display top 50 and bottom 10 words</a></div><div class=\"lev1 toc-item\"><a href=\"#HW3.3.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.3.-Shopping-Cart-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>HW3.3. Shopping Cart Analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#Largest-Basket\" data-toc-modified-id=\"Largest-Basket-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Largest Basket</a></div><div class=\"lev3 toc-item\"><a href=\"#Top-50-products\" data-toc-modified-id=\"Top-50-products-202\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Top 50 products</a></div><div class=\"lev3 toc-item\"><a href=\"#Number-of-unique-products\" data-toc-modified-id=\"Number-of-unique-products-203\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Number of unique products</a></div><div class=\"lev3 toc-item\"><a href=\"#There-are-12,592-unique-products.---The-largest-basket-contains-37-items.\" data-toc-modified-id=\"There-are-12,592-unique-products.---The-largest-basket-contains-37-items.-204\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>There are 12,592 unique products.   The largest basket contains 37 items.</a></div><div class=\"lev3 toc-item\"><a href=\"#3.3.1-OPTIONAL\" data-toc-modified-id=\"3.3.1-OPTIONAL-205\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>3.3.1 OPTIONAL</a></div><div class=\"lev3 toc-item\"><a href=\"#HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs\" data-toc-modified-id=\"HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs-206\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs</a></div><div class=\"lev3 toc-item\"><a href=\"#Count-&amp;-Sort-Pairs\" data-toc-modified-id=\"Count-&amp;-Sort-Pairs-207\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>Count &amp; Sort Pairs</a></div><div class=\"lev3 toc-item\"><a href=\"#HW3.5:-Stripes\" data-toc-modified-id=\"HW3.5:-Stripes-208\"><span class=\"toc-item-num\">2.0.8&nbsp;&nbsp;</span>HW3.5: Stripes</a></div><div class=\"lev2 toc-item\"><a href=\"#Pairs-vs-Stripes-comparison\" data-toc-modified-id=\"Pairs-vs-Stripes-comparison-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Pairs vs Stripes comparison</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW3\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  123457    __End of StudentId__     \n",
    "__Week:__   3\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time). I.e., Tuesday, Jan 31, 2017 in the case of this homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.0.\n",
    ">How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]   \n",
    "What is  a combiner function in the context of Hadoop?    \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.   \n",
    "What is the Hadoop shuffle?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge sort is a recursive algorithm that continually splits a list in half. If the list is empty or has one item, it is sorted by definition (the base case). If the list has more than one item, we split the list and recursively invoke a merge sort on both halves. Once the two halves are sorted, the fundamental operation, called a merge, is performed. Merging is the process of taking two smaller sorted lists and combining them together into a single, sorted, new list.\n",
    "\n",
    "__base case__: If the subarray size is 0 or 1, it is already sorted.   \n",
    "__recursive step__: Otherwise, compute m = lo + (hi - lo)/2, sort (recursively) the two subarrays a[lo, m) and a[m, hi), and merge them to produce a sorted result.     \n",
    "Source: http://interactivepython.org/runestone/static/pythonds/SortSearch/TheMergeSort.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Sort Illustrated \n",
    "\n",
    "<img src=\"http://interactivepython.org/runestone/static/pythonds/_images/mergesortA.png\"/>\n",
    "<img src=\"http://interactivepython.org/runestone/static/pythonds/_images/mergesortB.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce does a merge-sort-join of intermediate key-value pairs during the shuffle and sort phase.   \n",
    "*See cell below for simple implementation*   \n",
    "\n",
    "\n",
    "A __combiner__ is an optimization function which aggregates local data before sending it to the reducer. Hadoop makes no guarantees about utilizing this function, thus the correctness of the algorithm must not rely on it. One could use a combiner for doing a word count, where local word counts are calculated before being sent to the reducer. This improves perfomance by reducing network traffic, since there are fewer intermediate key/value pairs that need to be sent to the reducer. Ex) instead of two pairs (word,1) and (word,1), we can send one pair (word,2)\n",
    "\n",
    "The process of moving data from the mappers to the reducers is called __shuffling__. In this phase, Hadoop performs a groupBy operation using a hash function on the keys, placing Key/Value pairs with the same hash into the same reducer. Alternatively, the programer can specify a custom \"partition\" function which ensures that key/value parirs are sent to the same reducers based on some business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Merge-sort----------\n",
      "\n",
      "[17, 20, 26, 31, 44, 54, 55, 77, 93]\n",
      "\n",
      "--------Merge-join----------\n",
      "\n",
      "[1, 2, 3, 5, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "def mergeSortJoin(alist):\n",
    "    if len(alist)>1:\n",
    "        mid = len(alist)//2\n",
    "        lefthalf = alist[:mid]\n",
    "        righthalf = alist[mid:]\n",
    "\n",
    "        mergeSort(lefthalf)\n",
    "        mergeSort(righthalf)\n",
    "\n",
    "        i=0\n",
    "        j=0\n",
    "        k=0\n",
    "        while i < len(lefthalf) and j < len(righthalf):\n",
    "            if lefthalf[i] < righthalf[j]:\n",
    "                alist[k]=lefthalf[i]\n",
    "                i=i+1\n",
    "            else:\n",
    "                alist[k]=righthalf[j]\n",
    "                j=j+1\n",
    "            k=k+1\n",
    "\n",
    "        while i < len(lefthalf):\n",
    "            alist[k]=lefthalf[i]\n",
    "            i=i+1\n",
    "            k=k+1\n",
    "\n",
    "        while j < len(righthalf):\n",
    "            alist[k]=righthalf[j]\n",
    "            j=j+1\n",
    "            k=k+1\n",
    "    return alist\n",
    "\n",
    "\n",
    "\n",
    "# just the merge of two sorted lists\n",
    "def mergeJoin(lefthalf,righthalf):\n",
    "    \n",
    "    alist = [0 for i in range(len(lefthalf)+len(righthalf))]\n",
    "    i=0\n",
    "    j=0\n",
    "    k=0\n",
    "    while i < len(lefthalf) and j < len(righthalf):\n",
    "        if lefthalf[i] < righthalf[j]:\n",
    "            alist[k]=lefthalf[i]\n",
    "            i=i+1\n",
    "        else:\n",
    "            alist[k]=righthalf[j]\n",
    "            j=j+1\n",
    "        k=k+1\n",
    "\n",
    "    while i < len(lefthalf):\n",
    "        alist[k]=lefthalf[i]\n",
    "        i=i+1\n",
    "        k=k+1\n",
    "\n",
    "    while j < len(righthalf):\n",
    "        alist[k]=righthalf[j]\n",
    "        j=j+1\n",
    "        k=k+1\n",
    "    return alist\n",
    "\n",
    "print \"\\n--------Merge-sort----------\\n\"\n",
    "alist = [54,26,93,17,77,31,44,55,20]\n",
    "print mergeSortJoin(alist)\n",
    "\n",
    "print \"\\n--------Merge-join----------\\n\"\n",
    "A = [1, 3, 5, 5, 7]\n",
    "B = [2, 6, 8]\n",
    "print mergeJoin(A,B)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 \n",
    ">__consumer complaints dataset:__ Use Counters to do EDA (exploratory data analysis and to monitor progress)   \n",
    "\n",
    ">Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    ">While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    ">Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    ">     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    ">The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    ">Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to >company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    ">Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    ">Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    ">1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    ">User-defined Counters\n",
    "\n",
    ">Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    ">Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some directories\n",
    "!hdfs dfs -mkdir /user/koza/hw3\n",
    "!hdfs dfs -mkdir /user/koza/hw3/3.1\n",
    "!hdfs dfs -mkdir /user/koza/hw3/3.1/complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the data into HDFS\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/koza/hw3/3.1/complaints/Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaintCountsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsMapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    row_id, typeOfComplaint, restOfLine = line.split(',',2)\n",
    "    if typeOfComplaint == \"Debt collection\":\n",
    "        sys.stderr.write(\"reporter:counter:Mapper_DebtCollection,Debt,1\\n\")\n",
    "    elif typeOfComplaint == \"Mortgage\":\n",
    "        sys.stderr.write(\"reporter:counter:Mapper_Mortgage,Mortgage,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Mapper_Other,Other,1\\n\")\n",
    "    print '%s\\t%s' % (typeOfComplaint, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaintCountsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/02 21:53:15 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.1/output\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar6372515831079277973/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob3032339017704722984.jar tmpDir=null\n",
      "16/06/02 21:53:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/02 21:53:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/02 21:53:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 21:53:17 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/02 21:53:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0206\n",
      "16/06/02 21:53:17 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0206\n",
      "16/06/02 21:53:17 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0206/\n",
      "16/06/02 21:53:17 INFO mapreduce.Job: Running job: job_1464572970182_0206\n",
      "16/06/02 21:53:23 INFO mapreduce.Job: Job job_1464572970182_0206 running in uber mode : false\n",
      "16/06/02 21:53:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 21:53:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 21:53:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 21:53:36 INFO mapreduce.Job: Job job_1464572970182_0206 completed successfully\n",
      "16/06/02 21:53:36 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5504154\n",
      "\t\tFILE: Number of bytes written=11371778\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910834\n",
      "\t\tHDFS: Number of bytes written=194\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8459\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3432\n",
      "\t\tTotal time spent by all map tasks (ms)=8459\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3432\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8459\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3432\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8662016\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3514368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=5504160\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=5504160\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=146\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=585105408\n",
      "\tMapper_DebtCollection\n",
      "\t\tDebt=44372\n",
      "\tMapper_Mortgage\n",
      "\t\tMortgage=125752\n",
      "\tMapper_Other\n",
      "\t\tOther=142789\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=194\n",
      "16/06/02 21:53:36 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.1/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.1/output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1\" \\\n",
    "    -files complaintCountsMapper.py,complaintCountsReducer.py \\\n",
    "    -mapper complaintCountsMapper.py \\\n",
    "    -reducer complaintCountsReducer.py \\\n",
    "    -input /user/koza/hw3/3.1/complaints/Consumer_Complaints.csv -output /user/koza/hw3/3.1/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank account or service\t38073\r\n",
      "Consumer loan\t9387\r\n",
      "Credit card\t41563\r\n",
      "Credit reporting\t41214\r\n",
      "Debt collection\t44372\r\n",
      "Money transfers\t1540\r\n",
      "Mortgage\t125752\r\n",
      "Payday loan\t1579\r\n",
      "Product\t1\r\n",
      "Student loan\t9432\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/koza/hw3/3.1/output/* > complaintCounts.tsv\n",
    "!hdfs dfs -cat /user/koza/hw3/3.1/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAF4CAYAAAB3tt9kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117593750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_table('complaintCounts.tsv',header=None)\n",
    "\n",
    "    \n",
    "complaints = (data[0])\n",
    "y_pos = np.arange(len(objects))\n",
    "counts = data[1]\n",
    " \n",
    "plt.bar(y_pos, counts, align='center')\n",
    "plt.xticks(y_pos, complaints,rotation='vertical')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Complaint')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![\"counters\"](http://candpgeneration.com/images/counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    ">For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    ">Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    ">Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    ">Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: \n",
    ">- What are the top 50 most frequent terms in your word count analysis? \n",
    ">- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. \n",
    ">- Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solutions Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    for word in line.split():\n",
    "        print('%s\\t%d' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" > text.3.2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/koza/hw3/3.2\n",
    "!hdfs dfs -mkdir /user/koza/hw3/3.2/input\n",
    "!hdfs dfs -mkdir /user/koza/hw3/3.2/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put text.3.2.txt /user/koza/hw3/3.2/input/text.3.2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/02 22:39:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2/output\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar2147012108620711282/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob1756077623154200669.jar tmpDir=null\n",
      "16/06/02 22:39:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/02 22:39:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/02 22:39:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 22:39:43 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 22:39:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0209\n",
      "16/06/02 22:39:43 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0209\n",
      "16/06/02 22:39:43 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0209/\n",
      "16/06/02 22:39:43 INFO mapreduce.Job: Running job: job_1464572970182_0209\n",
      "16/06/02 22:39:49 INFO mapreduce.Job: Job job_1464572970182_0209 running in uber mode : false\n",
      "16/06/02 22:39:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 22:39:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 22:39:59 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/06/02 22:40:00 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 22:40:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 22:40:02 INFO mapreduce.Job: Job job_1464572970182_0209 completed successfully\n",
      "16/06/02 22:40:02 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=605198\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=141\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2455\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12652\n",
      "\t\tTotal time spent by all map tasks (ms)=2455\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12652\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2455\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12652\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2513920\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12955648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=186\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=1006632960\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/06/02 22:40:02 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.2/output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-files mapper3.2.py,reducer3.2.py \\\n",
    "-mapper mapper3.2.py \\\n",
    "-reducer reducer3.2.py \\\n",
    "-numReduceTasks 4 \\\n",
    "-input /user/koza/hw3/3.2/input/text.3.2.txt -output /user/koza/hw3/3.2/output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\n",
      "foo\t3\n",
      "quux\t2\n",
      "labs\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/koza/hw3/3.2/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"counters 3.2\"](http://candpgeneration.com/images/counters3.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "It would seem advatageous to use as many reducers as there are keys. This will ensure maximum parallelization, as each key is sent to it's own reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solutions Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperIssues.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperIssues.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in csv.reader(sys.stdin, delimiter=',', quotechar='\"'):\n",
    "    words = WORD_RE.findall(line[3])\n",
    "    for word in words:\n",
    "        print '%s\\t%d' %(word.lower(),1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerIssues.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerIssues.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/31 06:15:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2/issues/wordCount_part2\n",
      "16/05/31 06:15:02 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapperIssues.py, reducerIssues.py, /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar1826766945556797700/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob8200921857490816696.jar tmpDir=null\n",
      "16/05/31 06:15:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/31 06:15:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/31 06:15:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/31 06:15:03 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/31 06:15:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/31 06:15:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0073\n",
      "16/05/31 06:15:04 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0073\n",
      "16/05/31 06:15:04 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0073/\n",
      "16/05/31 06:15:04 INFO mapreduce.Job: Running job: job_1464572970182_0073\n",
      "16/05/31 06:15:10 INFO mapreduce.Job: Job job_1464572970182_0073 running in uber mode : false\n",
      "16/05/31 06:15:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/31 06:15:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/31 06:15:24 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/05/31 06:15:25 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/31 06:15:25 INFO mapreduce.Job: Job job_1464572970182_0073 completed successfully\n",
      "16/05/31 06:15:25 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16121345\n",
      "\t\tFILE: Number of bytes written=32728220\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910834\n",
      "\t\tHDFS: Number of bytes written=2174\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10569\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8737\n",
      "\t\tTotal time spent by all map tasks (ms)=10569\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8737\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10569\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8737\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10822656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8946688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348309\n",
      "\t\tMap output bytes=13424715\n",
      "\t\tMap output materialized bytes=16121357\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=16121357\n",
      "\t\tReduce input records=1348309\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=2696618\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=218\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=776994816\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2174\n",
      "16/05/31 06:15:25 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2/issues/wordCount_part2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.2/issues/wordCount_part2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-numReduceTasks 2 \\\n",
    "-file mapperIssues.py    -mapper mapperIssues.py \\\n",
    "-file reducerIssues.py   -reducer reducerIssues.py \\\n",
    "-input /user/koza/hw3/3.1/complaints/Consumer_Complaints.csv -output /user/koza/hw3/3.2/issues/wordCount_part2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 koza supergroup          0 2016-05-31 06:15 /user/koza/hw3/3.2/issues/wordCount_part2/_SUCCESS\n",
      "-rw-r--r--   1 koza supergroup       1031 2016-05-31 06:15 /user/koza/hw3/3.2/issues/wordCount_part2/part-00000\n",
      "-rw-r--r--   1 koza supergroup       1143 2016-05-31 06:15 /user/koza/hw3/3.2/issues/wordCount_part2/part-00001\n",
      "\n",
      "-------Partially sorted output--------------\n",
      "\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "an\t2964\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "action\t2964\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "application\t8868\n",
      "apply\t118\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t17972\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/koza/hw3/3.2/issues/wordCount_part2\n",
    "print \"\\n-------Partially sorted output--------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part2/part-00000 | head\n",
    "print \"\\n--------------------------------------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part2/part-00001 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"counters 3.2\"](http://candpgeneration.com/images/counters3.2.b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Solutions Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Count words with combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperIssues.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperIssues.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in csv.reader(sys.stdin, delimiter=',', quotechar='\"'):\n",
    "    words = WORD_RE.findall(line[3])\n",
    "    for word in words:\n",
    "        print '%s\\t%d' %(word.lower(),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combinerIssues.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerIssues.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerIssues.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerIssues.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/31 06:17:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2/issues/wordCount_part3\n",
      "16/05/31 06:17:43 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapperIssues.py, reducerIssues.py, combinerIssues.py, /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar8128912609811617649/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob3386687348524473402.jar tmpDir=null\n",
      "16/05/31 06:17:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/31 06:17:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/31 06:17:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/31 06:17:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/31 06:17:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0074\n",
      "16/05/31 06:17:44 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0074\n",
      "16/05/31 06:17:44 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0074/\n",
      "16/05/31 06:17:44 INFO mapreduce.Job: Running job: job_1464572970182_0074\n",
      "16/05/31 06:17:50 INFO mapreduce.Job: Job job_1464572970182_0074 running in uber mode : false\n",
      "16/05/31 06:17:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/31 06:17:59 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/31 06:18:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/31 06:18:04 INFO mapreduce.Job: Job job_1464572970182_0074 completed successfully\n",
      "16/05/31 06:18:04 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4651\n",
      "\t\tFILE: Number of bytes written=374425\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910834\n",
      "\t\tHDFS: Number of bytes written=2174\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11973\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2539\n",
      "\t\tTotal time spent by all map tasks (ms)=11973\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2539\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11973\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2539\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12260352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2599936\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348309\n",
      "\t\tMap output bytes=13424715\n",
      "\t\tMap output materialized bytes=4657\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=1348309\n",
      "\t\tCombine output records=324\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=4657\n",
      "\t\tReduce input records=324\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=648\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=102\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tCombiner Counters\n",
      "\t\tCalls=2\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2174\n",
      "16/05/31 06:18:04 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2/issues/wordCount_part3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.2/issues/wordCount_part3\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -file mapperIssues.py -mapper mapperIssues.py \\\n",
    "    -file reducerIssues.py -reducer reducerIssues.py \\\n",
    "    -file combinerIssues.py -combiner combinerIssues.py \\\n",
    "    -input /user/koza/hw3/3.1/complaints/Consumer_Complaints.csv -output /user/koza/hw3/3.2/issues/wordCount_part3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"counters 3.2\"](http://candpgeneration.com/images/counters3.2.combiners.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t3503\r\n",
      "account\t57448\r\n",
      "acct\t163\r\n",
      "action\t2964\r\n",
      "advance\t240\r\n",
      "advertising\t1193\r\n",
      "amount\t98\r\n",
      "amt\t71\r\n",
      "an\t2964\r\n",
      "and\t16448\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part3/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Calculate Relative Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequenciesMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequenciesMapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "total = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    total += int(value)  \n",
    "    print \"%s\\t%s\" % (key,value)\n",
    "print \"!TOTAL\\t\",total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequenciesReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequenciesReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == \"!TOTAL\":\n",
    "        total += int(value)\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\" % (value, str(int(value)/total), key) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/31 06:18:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2/issues/frequencies_part3\n",
      "16/05/31 06:18:15 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [frequenciesMapper.py, frequenciesReducer.py, /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar8824481523797164892/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob1092866091374647063.jar tmpDir=null\n",
      "16/05/31 06:18:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/31 06:18:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/31 06:18:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/31 06:18:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/31 06:18:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0075\n",
      "16/05/31 06:18:16 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0075\n",
      "16/05/31 06:18:16 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0075/\n",
      "16/05/31 06:18:16 INFO mapreduce.Job: Running job: job_1464572970182_0075\n",
      "16/05/31 06:18:22 INFO mapreduce.Job: Job job_1464572970182_0075 running in uber mode : false\n",
      "16/05/31 06:18:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/31 06:18:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/31 06:18:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/31 06:18:32 INFO mapreduce.Job: Job job_1464572970182_0075 completed successfully\n",
      "16/05/31 06:18:32 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2736\n",
      "\t\tFILE: Number of bytes written=371093\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3511\n",
      "\t\tHDFS: Number of bytes written=5152\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5547\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2542\n",
      "\t\tTotal time spent by all map tasks (ms)=5547\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2542\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5547\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2542\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5680128\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2603008\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=174\n",
      "\t\tMap output records=176\n",
      "\t\tMap output bytes=2378\n",
      "\t\tMap output materialized bytes=2742\n",
      "\t\tInput split bytes=250\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=176\n",
      "\t\tReduce shuffle bytes=2742\n",
      "\t\tReduce input records=176\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=352\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=72\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3261\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5152\n",
      "16/05/31 06:18:32 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2/issues/frequencies_part3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.2/issues/frequencies_part3\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D stream.num.map.output.key.field=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -file frequenciesMapper.py -mapper frequenciesMapper.py \\\n",
    "    -file frequenciesReducer.py -reducer frequenciesReducer.py \\\n",
    "    -input /user/koza/hw3/3.2/issues/wordCount_part3/* -output /user/koza/hw3/3.2/issues/frequencies_part3 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------Top 50-----------\n",
      "\n",
      "count   frequency       word\n",
      "----------------------------\n",
      "\n",
      "119630\t0.0887259522854\tloan\n",
      "72394\t0.0536924399377\tcollection\n",
      "70487\t0.052278075723\tforeclosure\n",
      "70487\t0.052278075723\tmodification\n",
      "57448\t0.0426074438426\taccount\n",
      "55251\t0.0409779954002\tcredit\n",
      "40508\t0.0300435582645\tor\n",
      "39993\t0.0296615983428\tpayments\n",
      "36767\t0.0272689717268\tescrow\n",
      "36767\t0.0272689717268\tservicing\n",
      "34903\t0.0258864993114\treport\n",
      "29133\t0.0216070648494\tincorrect\n",
      "29069\t0.0215595979853\tinformation\n",
      "29069\t0.0215595979853\ton\n",
      "27874\t0.0206733026332\tdebt\n",
      "19000\t0.0140917252648\tclosing\n",
      "18477\t0.0137038319851\tnot\n",
      "17972\t0.013329288761\tattempts\n",
      "17972\t0.013329288761\tcollect\n",
      "17972\t0.013329288761\tcont'd\n",
      "17972\t0.013329288761\towed\n",
      "16448\t0.0121989840608\tand\n",
      "16205\t0.0120187583113\tmanagement\n",
      "16205\t0.0120187583113\topening\n",
      "13983\t0.0103707681251\tof\n",
      "10731\t0.00795885809558\tmy\n",
      "10555\t0.00782832421945\tdeposits\n",
      "10555\t0.00782832421945\twithdrawals\n",
      "9484\t0.00703399591637\tproblems\n",
      "8868\t0.00657712734989\tapplication\n",
      "8671\t0.00643101840898\tcommunication\n",
      "8671\t0.00643101840898\ttactics\n",
      "8625\t0.00639690160045\tbroker\n",
      "8625\t0.00639690160045\tmortgage\n",
      "8625\t0.00639690160045\toriginator\n",
      "8401\t0.00623076757628\tto\n",
      "8178\t0.00606537522185\tunable\n",
      "8158\t0.00605054182684\tbilling\n",
      "7886\t0.00584880765463\tother\n",
      "7655\t0.0056774819422\tdisclosure\n",
      "7655\t0.0056774819422\tverification\n",
      "6938\t0.00514570473089\tdisputes\n",
      "6559\t0.00486461189534\treporting\n",
      "6337\t0.00469996121067\tlease\n",
      "6248\t0.00463395260285\tthe\n",
      "5663\t0.00420007579865\tbeing\n",
      "5663\t0.00420007579865\tby\n",
      "5663\t0.00420007579865\tcaused\n",
      "5663\t0.00420007579865\tfunds\n",
      "5663\t0.00420007579865\tlow\n",
      "\n",
      "--------Bottom 10---------\n",
      "\n",
      "118\t8.75170305917e-05\tapply\n",
      "98\t7.26836355761e-05\tamount\n",
      "92\t6.82336170715e-05\tcredited\n",
      "92\t6.82336170715e-05\tpayment\n",
      "75\t5.56252313083e-05\tchecks\n",
      "75\t5.56252313083e-05\tconvenience\n",
      "71\t5.26585523051e-05\tamt\n",
      "71\t5.26585523051e-05\tday\n",
      "64\t4.74668640497e-05\tdisclosures\n",
      "64\t4.74668640497e-05\tmissing\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | head \n",
    "print \"\\n-----------Top 50-----------\\n\"\n",
    "print \"count   frequency       word\"\n",
    "print \"----------------------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | head -n 50\n",
    "print \"\\n--------Bottom 10---------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119630\t0.0887259522854\tloan\r\n",
      "72394\t0.0536924399377\tcollection\r\n",
      "70487\t0.052278075723\tforeclosure\r\n",
      "70487\t0.052278075723\tmodification\r\n",
      "57448\t0.0426074438426\taccount\r\n",
      "55251\t0.0409779954002\tcredit\r\n",
      "40508\t0.0300435582645\tor\r\n",
      "39993\t0.0296615983428\tpayments\r\n",
      "36767\t0.0272689717268\tescrow\r\n",
      "36767\t0.0272689717268\tservicing\r\n",
      "34903\t0.0258864993114\treport\r\n",
      "29133\t0.0216070648494\tincorrect\r\n",
      "29069\t0.0215595979853\tinformation\r\n",
      "29069\t0.0215595979853\ton\r\n",
      "27874\t0.0206733026332\tdebt\r\n",
      "19000\t0.0140917252648\tclosing\r\n",
      "18477\t0.0137038319851\tnot\r\n",
      "17972\t0.013329288761\tattempts\r\n",
      "17972\t0.013329288761\tcollect\r\n",
      "17972\t0.013329288761\tcont'd\r\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: sort with commandline\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | sort -k1,1nr -k3,3 | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1  \n",
    ">Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Solution\n",
    "- __Step 1__. It is at the word count stage that the combiner is utilized. The reason to use a combiner is to do as much local aggregation as possible, in order to reduce the amount of data sent to the reducers, which also reduces the amount of computation at the reducer stage. No combiner is necessary after this point, as we are not doing any more aggregation.\n",
    "\n",
    "\n",
    "- __Step 2__. To use two reducers, we need to partition the data in two in such a way as to ensure Total Order Sort. We'll first inspect the distribution of the counts to determine the best split point in order to distribute the data as evenly as possible across the reducers. \n",
    "\n",
    "\n",
    "- __Step 3__. Write a mapper that partitions the data using the information from step 2.\n",
    "\n",
    "\n",
    "- __Step 4__. Using hadoop, sort the data, using the partitioner mapper, and two reducers. We can take advantage of hadoop's options to remove the partition key and display only the columns of interest, namely, the Count, Frequency, and Word.\n",
    "\n",
    "\n",
    "- __Step 5__. This last step simply displays the head and tail of the approriate partitions. We could combine these two files, but for the purposes of the exercise, this is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Get counts. This was done in above section, hense reusing the word count output from hw3.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word    count\n",
      "----------------------------\n",
      "\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "action\t2964\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "an\t2964\n",
      "and\t16448\n"
     ]
    }
   ],
   "source": [
    "# Inspect counts\n",
    "print \"word    count\"\n",
    "print \"----------------------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part3/* | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Inspect the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116efb510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\t7748.90229885\n",
      "median:\t3081.0\n",
      "max:\t119630\n"
     ]
    }
   ],
   "source": [
    "# In order to partition the counts evenly, look at the distribution of the counts. \n",
    "# Since it is highly skewed, we'll use median.\n",
    "# given that the data fits in memory, we can get the median using the whole dataset\n",
    "# otherwise we would take a sample.\n",
    "\n",
    "\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cat\",\n",
    "                      \"/user/koza/hw3/3.2/issues/wordCount_part3/part-00000\"],\n",
    "                      stdout=subprocess.PIPE)\n",
    "counts = []\n",
    "for line in p.stdout.readlines():\n",
    "        line = line.strip()\n",
    "        cols = line.split(\"\\t\")\n",
    "        counts.append(int(cols[1]))\n",
    "        \n",
    "plt.hist(counts)\n",
    "plt.show()\n",
    "\n",
    "print \"mean:\\t\",np.mean(np.array(counts))\n",
    "print \"median:\\t\",np.median(np.array(counts))   \n",
    "print \"max:\\t\",np.max(np.array(counts)) \n",
    "\n",
    "COUNTS_MEDIAN = np.median(np.array(counts))\n",
    "\n",
    "# Save summary satistics to file for later use\n",
    "file = open(\"Counts.txt\", \"w\")\n",
    "file.write(str(COUNTS_MEDIAN))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081.0\n"
     ]
    }
   ],
   "source": [
    "# CHECK THAT THE COUNTS FILE CONTAINS CORRECT NUMBER\n",
    "\n",
    "file = open('Counts.txt', 'r')\n",
    "line = file.readline()\n",
    "line = line.strip()\n",
    "COUNTS_MEDIAN = float(line)\n",
    "print COUNTS_MEDIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Partition the counts into 2 groups for use with 2 reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prependPartitionKeyMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prependPartitionKeyMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "file = open('Counts.txt', 'r')\n",
    "line = file.readline()\n",
    "line = line.strip()\n",
    "\n",
    "COUNTS_MEDIAN = float(line)\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    total += int(value)\n",
    "    \n",
    "    \n",
    "    partitionKey=\"b\"     \n",
    "    if int(value) < COUNTS_MEDIAN:\n",
    "        partitionKey = \"a\"        \n",
    "      \n",
    "    print \"%s\\t%s\\t%s\" % (partitionKey, value, key)\n",
    "\n",
    "###########################################################\n",
    "# Since the total is going to be the larset value\n",
    "# it is guaranteed to appear first in the reducers\n",
    "# when we do a reverse sort.\n",
    "\n",
    "# we make it available in both partitions by prepending\n",
    "# each of the partition keys\n",
    "###########################################################\n",
    "\n",
    "print \"%s\\t%s\\t%s\" % (\"a\",total,\"!total\")    \n",
    "print \"%s\\t%s\\t%s\" % (\"b\",total,\"!total\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prependPartitionKeyReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prependPartitionKeyReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    cols = line.split(\"\\t\")\n",
    "    \n",
    "    if cols[2] == \"!total\":\n",
    "        total += int(cols[1])\n",
    "    else:\n",
    "        # drop the partition key, by not including col[0]  \n",
    "        print \"%s\\t%s\\t%s\" % (cols[1], str(int(cols[1])/total), cols[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 14:59:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2/issues/frequencies_part5_sorted\n",
      "16/06/05 14:59:38 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/06/05 14:59:38 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/06/05 14:59:38 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/06/05 14:59:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 14:59:38 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/05 14:59:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1539118734_0001\n",
      "16/06/05 14:59:39 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/koza/Documents/UCBerkeley/261/HW3-Questions/prependPartitionKeyMapper.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1465153179161/prependPartitionKeyMapper.py\n",
      "16/06/05 14:59:39 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/koza/Documents/UCBerkeley/261/HW3-Questions/prependPartitionKeyReducer.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1465153179162/prependPartitionKeyReducer.py\n",
      "16/06/05 14:59:39 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/koza/Documents/UCBerkeley/261/HW3-Questions/Counts.txt as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1465153179163/Counts.txt\n",
      "16/06/05 14:59:39 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/06/05 14:59:39 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/06/05 14:59:39 INFO mapreduce.Job: Running job: job_local1539118734_0001\n",
      "16/06/05 14:59:39 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/06/05 14:59:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/06/05 14:59:39 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/06/05 14:59:39 INFO mapred.LocalJobRunner: Starting task: attempt_local1539118734_0001_m_000000_0\n",
      "16/06/05 14:59:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/06/05 14:59:39 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/06/05 14:59:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/koza/hw3/3.2/issues/wordCount_part3/part-00000:0+2174\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/koza/Documents/UCBerkeley/261/HW3-Questions/./prependPartitionKeyMapper.py]\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/06/05 14:59:39 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: Records R/W=174/1\n",
      "16/06/05 14:59:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/06/05 14:59:39 INFO mapred.LocalJobRunner: \n",
      "16/06/05 14:59:39 INFO mapred.MapTask: Starting flush of map output\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: Spilling map output\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: bufstart = 0; bufend = 2732; bufvoid = 104857600\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213696(104854784); length = 701/6553600\n",
      "16/06/05 14:59:39 INFO mapred.MapTask: Finished spill 0\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task:attempt_local1539118734_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Records R/W=174/1\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task 'attempt_local1539118734_0001_m_000000_0' done.\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1539118734_0001_m_000000_0\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1539118734_0001_r_000000_0\n",
      "16/06/05 14:59:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/06/05 14:59:40 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/06/05 14:59:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/06/05 14:59:40 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33d4cadc\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/06/05 14:59:40 INFO reduce.EventFetcher: attempt_local1539118734_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/06/05 14:59:40 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1539118734_0001_m_000000_0 decomp: 1580 len: 1584 to MEMORY\n",
      "16/06/05 14:59:40 INFO reduce.InMemoryMapOutput: Read 1580 bytes from map-output for attempt_local1539118734_0001_m_000000_0\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1580, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1580\n",
      "16/06/05 14:59:40 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1561 bytes\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: Merged 1 segments, 1580 bytes to disk to satisfy reduce memory limit\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: Merging 1 files, 1584 bytes from disk\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1561 bytes\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/koza/Documents/UCBerkeley/261/HW3-Questions/./prependPartitionKeyReducer.py]\n",
      "16/06/05 14:59:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/05 14:59:40 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: Records R/W=88/1\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task:attempt_local1539118734_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task attempt_local1539118734_0001_r_000000_0 is allowed to commit now\n",
      "16/06/05 14:59:40 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1539118734_0001_r_000000_0' to hdfs://localhost:9000/user/koza/hw3/3.2/issues/frequencies_part5_sorted/_temporary/0/task_local1539118734_0001_r_000000\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Records R/W=88/1 > reduce\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task 'attempt_local1539118734_0001_r_000000_0' done.\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1539118734_0001_r_000000_0\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1539118734_0001_r_000001_0\n",
      "16/06/05 14:59:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/06/05 14:59:40 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/06/05 14:59:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/06/05 14:59:40 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3ad74455\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/06/05 14:59:40 INFO reduce.EventFetcher: attempt_local1539118734_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/06/05 14:59:40 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1539118734_0001_m_000000_0 decomp: 1508 len: 1512 to MEMORY\n",
      "16/06/05 14:59:40 INFO reduce.InMemoryMapOutput: Read 1508 bytes from map-output for attempt_local1539118734_0001_m_000000_0\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1508, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1508\n",
      "16/06/05 14:59:40 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1489 bytes\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: Merged 1 segments, 1508 bytes to disk to satisfy reduce memory limit\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: Merging 1 files, 1512 bytes from disk\n",
      "16/06/05 14:59:40 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/06/05 14:59:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1489 bytes\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/koza/Documents/UCBerkeley/261/HW3-Questions/./prependPartitionKeyReducer.py]\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: Records R/W=88/1\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/06/05 14:59:40 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task:attempt_local1539118734_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task attempt_local1539118734_0001_r_000001_0 is allowed to commit now\n",
      "16/06/05 14:59:40 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1539118734_0001_r_000001_0' to hdfs://localhost:9000/user/koza/hw3/3.2/issues/frequencies_part5_sorted/_temporary/0/task_local1539118734_0001_r_000001\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Records R/W=88/1 > reduce\n",
      "16/06/05 14:59:40 INFO mapred.Task: Task 'attempt_local1539118734_0001_r_000001_0' done.\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1539118734_0001_r_000001_0\n",
      "16/06/05 14:59:40 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/06/05 14:59:40 INFO mapreduce.Job: Job job_local1539118734_0001 running in uber mode : false\n",
      "16/06/05 14:59:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 14:59:40 INFO mapreduce.Job: Job job_local1539118734_0001 completed successfully\n",
      "16/06/05 14:59:40 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=334341\n",
      "\t\tFILE: Number of bytes written=1243608\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6522\n",
      "\t\tHDFS: Number of bytes written=7721\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=174\n",
      "\t\tMap output records=176\n",
      "\t\tMap output bytes=2732\n",
      "\t\tMap output materialized bytes=3096\n",
      "\t\tInput split bytes=125\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=176\n",
      "\t\tReduce shuffle bytes=3096\n",
      "\t\tReduce input records=176\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=352\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=975175680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2174\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5152\n",
      "16/06/05 14:59:40 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2/issues/frequencies_part5_sorted\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENTAL ##\n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.2/issues/frequencies_part5_sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k3,3\" \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -files prependPartitionKeyMapper.py,prependPartitionKeyReducer.py,Counts.txt \\\n",
    "    -mapper prependPartitionKeyMapper.py \\\n",
    "    -reducer prependPartitionKeyReducer.py \\\n",
    "    -input /user/koza/hw3/3.2/issues/wordCount_part3/* \\\n",
    "    -output /user/koza/hw3/3.2/issues/frequencies_part5_sorted \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. Display top 50 and bottom 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 koza supergroup          0 2016-06-05 14:59 /user/koza/hw3/3.2/issues/frequencies_part5_sorted/_SUCCESS\n",
      "-rw-r--r--   1 koza supergroup       2569 2016-06-05 14:59 /user/koza/hw3/3.2/issues/frequencies_part5_sorted/part-00000\n",
      "-rw-r--r--   1 koza supergroup       2583 2016-06-05 14:59 /user/koza/hw3/3.2/issues/frequencies_part5_sorted/part-00001\n",
      "\n",
      "------------Top 50 words-------------\n",
      "\n",
      "Count   Frequency       Word\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "119630\t0.0887259522854\tloan\n",
      "72394\t0.0536924399377\tcollection\n",
      "70487\t0.052278075723\tforeclosure\n",
      "70487\t0.052278075723\tmodification\n",
      "57448\t0.0426074438426\taccount\n",
      "55251\t0.0409779954002\tcredit\n",
      "40508\t0.0300435582645\tor\n",
      "39993\t0.0296615983428\tpayments\n",
      "36767\t0.0272689717268\tescrow\n",
      "36767\t0.0272689717268\tservicing\n",
      "34903\t0.0258864993114\treport\n",
      "29133\t0.0216070648494\tincorrect\n",
      "29069\t0.0215595979853\tinformation\n",
      "29069\t0.0215595979853\ton\n",
      "27874\t0.0206733026332\tdebt\n",
      "19000\t0.0140917252648\tclosing\n",
      "18477\t0.0137038319851\tnot\n",
      "17972\t0.013329288761\tattempts\n",
      "17972\t0.013329288761\tcollect\n",
      "17972\t0.013329288761\tcont'd\n",
      "17972\t0.013329288761\towed\n",
      "16448\t0.0121989840608\tand\n",
      "16205\t0.0120187583113\tmanagement\n",
      "16205\t0.0120187583113\topening\n",
      "13983\t0.0103707681251\tof\n",
      "10731\t0.00795885809558\tmy\n",
      "10555\t0.00782832421945\tdeposits\n",
      "10555\t0.00782832421945\twithdrawals\n",
      "9484\t0.00703399591637\tproblems\n",
      "8868\t0.00657712734989\tapplication\n",
      "8671\t0.00643101840898\tcommunication\n",
      "8671\t0.00643101840898\ttactics\n",
      "8625\t0.00639690160045\tbroker\n",
      "8625\t0.00639690160045\tmortgage\n",
      "8625\t0.00639690160045\toriginator\n",
      "8401\t0.00623076757628\tto\n",
      "8178\t0.00606537522185\tunable\n",
      "8158\t0.00605054182684\tbilling\n",
      "7886\t0.00584880765463\tother\n",
      "7655\t0.0056774819422\tdisclosure\n",
      "7655\t0.0056774819422\tverification\n",
      "6938\t0.00514570473089\tdisputes\n",
      "6559\t0.00486461189534\treporting\n",
      "6337\t0.00469996121067\tlease\n",
      "6248\t0.00463395260285\tthe\n",
      "5663\t0.00420007579865\tbeing\n",
      "5663\t0.00420007579865\tby\n",
      "5663\t0.00420007579865\tcaused\n",
      "5663\t0.00420007579865\tfunds\n",
      "5663\t0.00420007579865\tlow\n",
      "\n",
      "----------Bottom 10 words------------\n",
      "\n",
      "118\t8.75170305917e-05\tapply\n",
      "98\t7.26836355761e-05\tamount\n",
      "92\t6.82336170715e-05\tcredited\n",
      "92\t6.82336170715e-05\tpayment\n",
      "75\t5.56252313083e-05\tchecks\n",
      "75\t5.56252313083e-05\tconvenience\n",
      "71\t5.26585523051e-05\tamt\n",
      "71\t5.26585523051e-05\tday\n",
      "64\t4.74668640497e-05\tdisclosures\n",
      "64\t4.74668640497e-05\tmissing\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/koza/hw3/3.2/issues/frequencies_part5_sorted\n",
    "print \"\\n------------Top 50 words-------------\\n\"\n",
    "print \"Count   Frequency       Word\"\n",
    "print \"\\n-------------------------------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part5_sorted/part-00000 | head -n 50\n",
    "print \"\\n----------Bottom 10 words------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part5_sorted/part-00001 | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3.3. Shopping Cart Analysis\n",
    ">Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    ">For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    ">Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    ">Here are the first few lines of the ProductPurchaseData    \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222   \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192   \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643    \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465   \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444   \n",
    "\n",
    "\n",
    ">Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    ">How many unique items are available from this supplier?\n",
    "\n",
    ">Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \r\n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \r\n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \r\n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO18919 DAI50921 SNA80192 GRO75578 \r\n",
      "ELE17451 ELE59935 FRO18919 ELE23393 SNA80192 SNA85662 SNA91554 DAI22177 \r\n",
      "ELE17451 SNA69641 FRO18919 SNA90258 ELE28573 ELE11375 DAI14125 FRO78087 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA80192 SNA85662 SNA90258 DAI46755 FRO81176 ELE66810 DAI49199 DAI91535 GRO94758 ELE94711 DAI22177 \r\n",
      "ELE17451 SNA69641 DAI91535 GRO94758 GRO99222 FRO76833 FRO81176 SNA80192 DAI54690 ELE37798 GRO56989 \r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the data\n",
    "!cat ProductPurchaseData.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productWordCountMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productWordCountMapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import re\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "max_len = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    for word in words:\n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Total,1\\n\")\n",
    "        print '%s\\t%d' % (word, 1)\n",
    "print \"!MAX_LENGTH\\t%s\" % (max_len)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productWordCountReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productWordCountReducer.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "max_len = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == \"!MAX_LENGTH\":\n",
    "        if int(value) > max_len:\n",
    "            max_len = int(value)\n",
    "    elif key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "print \"!MAX_LENGTH\\t%s\" % (max_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 23:30:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2.1/productWordCount\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar6871186165283007696/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob3915969876920185843.jar tmpDir=null\n",
      "16/05/29 23:30:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/29 23:30:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/29 23:30:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 23:30:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 23:30:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0025\n",
      "16/05/29 23:30:30 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0025\n",
      "16/05/29 23:30:30 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0025/\n",
      "16/05/29 23:30:30 INFO mapreduce.Job: Running job: job_1464572970182_0025\n",
      "16/05/29 23:30:36 INFO mapreduce.Job: Job job_1464572970182_0025 running in uber mode : false\n",
      "16/05/29 23:30:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 23:30:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 23:30:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 23:30:48 INFO mapreduce.Job: Job job_1464572970182_0025 completed successfully\n",
      "16/05/29 23:30:48 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4950752\n",
      "\t\tFILE: Number of bytes written=10264881\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462847\n",
      "\t\tHDFS: Number of bytes written=142673\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7908\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3456\n",
      "\t\tTotal time spent by all map tasks (ms)=7908\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3456\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7908\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3456\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8097792\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3538944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=4189094\n",
      "\t\tMap output materialized bytes=4950758\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=4950758\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12593\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=129\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=600309760\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\t\tTotal=380824\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142673\n",
      "16/05/29 23:30:48 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2.1/productWordCount\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.2.1/productWordCount\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "-files productWordCountMapper.py,productWordCountReducer.py \\\n",
    "-mapper productWordCountMapper.py \\\n",
    "-reducer productWordCountReducer.py \\\n",
    "-input /user/koza/hw3/3.2.1/ProductPurchaseData.txt -output /user/koza/hw3/3.2.1/productWordCount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest Basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!MAX_LENGTH\t37\r\n"
     ]
    }
   ],
   "source": [
    "# largest basket:\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2.1/productWordCount/* | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productFreqMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productFreqMapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == \"!MAX_LENGTH\":\n",
    "        continue\n",
    "    else:    \n",
    "        total += int(value)  \n",
    "        print \"%s\\t%s\" % (key,value)\n",
    "print \"!TOTAL\\t\",total  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productFreqReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productFreqReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == \"!TOTAL\":\n",
    "        total += int(value)\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\" % (key, value, str(int(value)/total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 23:31:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.2.1/productFrequencies\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar1065902891114653912/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob1197862960280895934.jar tmpDir=null\n",
      "16/05/29 23:31:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/29 23:31:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/29 23:31:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 23:31:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 23:31:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0026\n",
      "16/05/29 23:31:50 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0026\n",
      "16/05/29 23:31:50 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0026/\n",
      "16/05/29 23:31:50 INFO mapreduce.Job: Running job: job_1464572970182_0026\n",
      "16/05/29 23:31:56 INFO mapreduce.Job: Job job_1464572970182_0026 running in uber mode : false\n",
      "16/05/29 23:31:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 23:32:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 23:32:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 23:32:07 INFO mapreduce.Job: Job job_1464572970182_0026 completed successfully\n",
      "16/05/29 23:32:07 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180474\n",
      "\t\tFILE: Number of bytes written=726587\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=147011\n",
      "\t\tHDFS: Number of bytes written=368635\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6463\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2736\n",
      "\t\tTotal time spent by all map tasks (ms)=6463\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2736\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6463\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2736\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6618112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2801664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12593\n",
      "\t\tMap output records=12594\n",
      "\t\tMap output bytes=155280\n",
      "\t\tMap output materialized bytes=180480\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=180480\n",
      "\t\tReduce input records=12594\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25188\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=73\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=146769\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368635\n",
      "16/05/29 23:32:07 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.2.1/productFrequencies\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/hw3/3.2.1/productFrequencies\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "-D stream.num.map.output.key.field=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-files productFreqMapper.py,productFreqReducer.py \\\n",
    "-mapper productFreqMapper.py \\\n",
    "-reducer productFreqReducer.py \\\n",
    "-input /user/koza/hw3/3.2.1/productWordCount -output /user/koza/hw3/3.2.1/productFrequencies \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 50 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID      Count   Relative Frequency\n",
      "-------------------------------------------\n",
      "\n",
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "DAI22896\t1219\t0.0032009537214\n",
      "GRO85051\t1214\t0.00318782429679\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "print \"Product ID      Count   Relative Frequency\"\n",
    "print \"-------------------------------------------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12592\r\n"
     ]
    }
   ],
   "source": [
    "# Number of unique products\n",
    "!hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 12,592 unique products.   The largest basket contains 37 items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Similarly to 3.2.1, this can be acheived by partitioning the data first. Please see above.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    ">Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    ">List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    ">Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    ">Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    ">      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    ">Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    ">Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsMapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "############################################################\n",
    "# Using order inversion, we are able to count the total\n",
    "# number of baskets to send to the reducer for calculating\n",
    "# the relative frequencies.\n",
    "############################################################\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    total += 1\n",
    "    for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "        print '%s\\t%s' % (subset[0]+\" - \"+subset[1], 1)\n",
    "print \"%s\\t%s\" % (\"!TOTAL\", total)    \n",
    "\n",
    "############################################################\n",
    "# Not sure why the results from below algo are different   #\n",
    "############################################################\n",
    "#     j = 1\n",
    "#     for i in range(len(words)):\n",
    "#         for j in range(len(words)- i - 1):\n",
    "#             j += i+1\n",
    "#             print '%s\\t%d' % (words[i]+\"_\"+words[j], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "support_count = 100\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key,value = line.split(\"\\t\")\n",
    "    if key == \"!TOTAL\":\n",
    "        total += int(value)\n",
    "    elif key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            if cur_count >= support_count:\n",
    "                print '%s\\t%s\\t%s' % (cur_key,cur_count,str(cur_count/total))\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "if cur_count >= support_count:\n",
    "    print '%s\\t%s\\t%s' % (cur_key,cur_count,str(cur_count/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count & Sort Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/01 08:36:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.4/pairs\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar8706558061429981454/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob6454012764295061016.jar tmpDir=null\n",
      "16/06/01 08:36:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/01 08:36:57 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/01 08:36:57 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/01 08:36:57 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/01 08:36:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0139\n",
      "16/06/01 08:36:57 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0139\n",
      "16/06/01 08:36:57 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0139/\n",
      "16/06/01 08:36:57 INFO mapreduce.Job: Running job: job_1464572970182_0139\n",
      "16/06/01 08:37:04 INFO mapreduce.Job: Job job_1464572970182_0139 running in uber mode : false\n",
      "16/06/01 08:37:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/01 08:37:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/01 08:37:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/01 08:37:23 INFO mapreduce.Job: Job job_1464572970182_0139 completed successfully\n",
      "16/06/01 08:37:23 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=60816372\n",
      "\t\tFILE: Number of bytes written=121995755\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462847\n",
      "\t\tHDFS: Number of bytes written=54433\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12500\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8404\n",
      "\t\tTotal time spent by all map tasks (ms)=12500\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8404\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12500\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8404\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12800000\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8605696\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534016\n",
      "\t\tMap output bytes=55748334\n",
      "\t\tMap output materialized bytes=60816378\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=60816378\n",
      "\t\tReduce input records=2534016\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=5068032\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=71\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=54433\n",
      "16/06/01 08:37:23 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.4/pairs\n",
      "16/06/01 08:37:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.4/pairs_sorted\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar3382624823714736610/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob7544130462753765795.jar tmpDir=null\n",
      "16/06/01 08:37:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/01 08:37:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/01 08:37:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/01 08:37:27 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/01 08:37:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0140\n",
      "16/06/01 08:37:27 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0140\n",
      "16/06/01 08:37:27 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0140/\n",
      "16/06/01 08:37:27 INFO mapreduce.Job: Running job: job_1464572970182_0140\n",
      "16/06/01 08:37:34 INFO mapreduce.Job: Job job_1464572970182_0140 running in uber mode : false\n",
      "16/06/01 08:37:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/01 08:37:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/01 08:37:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/01 08:37:44 INFO mapreduce.Job: Job job_1464572970182_0140 completed successfully\n",
      "16/06/01 08:37:44 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=58441\n",
      "\t\tFILE: Number of bytes written=477742\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=58745\n",
      "\t\tHDFS: Number of bytes written=55767\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5537\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2476\n",
      "\t\tTotal time spent by all map tasks (ms)=5537\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2476\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5537\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2476\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5669888\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2535424\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=55767\n",
      "\t\tMap output materialized bytes=58447\n",
      "\t\tInput split bytes=216\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=58447\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=73\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=58529\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=55767\n",
      "16/06/01 08:37:44 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.4/pairs_sorted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Get counts:\n",
    "start = time.time()\n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.4/pairs\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -files pairsMapper.py,pairsReducer.py \\\n",
    "    -mapper pairsMapper.py \\\n",
    "    -reducer pairsReducer.py \\\n",
    "    -input /user/koza/hw3/3.2.1/ProductPurchaseData.txt -output /user/koza/hw3/3.4/pairs\n",
    "end = time.time()\n",
    "countTime = end-start\n",
    "\n",
    "\n",
    "\n",
    "# Do a secondary sort:\n",
    "start = time.time()\n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.4/pairs_sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D stream.num.map.output.key.field=2 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input /user/koza/hw3/3.4/pairs/* -output /user/koza/hw3/3.4/pairs_sorted\n",
    "\n",
    "end = time.time()\n",
    "sortTime = end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  Time to count and sort pairs  --------\n",
      "\n",
      "Count time:\t 29.448766 sec\n",
      "Sort time:\t 21.381309 sec\n",
      "Total time:\t 50.830075 sec \n",
      "\n",
      "------------- Most frequent pairs --------------\n",
      "\n",
      "\n",
      "Pair                Support count     Support \n",
      "\n",
      "DAI62779 - ELE17451\t1592\t0.0511880646925\t\n",
      "FRO40251 - SNA80324\t1412\t0.0454004694383\t\n",
      "DAI75645 - FRO40251\t1254\t0.0403202469374\t\n",
      "FRO40251 - GRO85051\t1213\t0.0390019613517\t\n",
      "DAI62779 - GRO73461\t1139\t0.0366226166361\t\n",
      "DAI75645 - SNA80324\t1130\t0.0363332368734\t\n",
      "DAI62779 - FRO40251\t1070\t0.0344040384554\t\n",
      "DAI62779 - SNA80324\t923\t0.0296775023311\t\n",
      "DAI62779 - DAI85309\t918\t0.0295167357963\t\n",
      "ELE32164 - GRO59710\t911\t0.0292916626475\t\n",
      "DAI62779 - DAI75645\t882\t0.0283592167454\t\n",
      "FRO40251 - GRO73461\t882\t0.0283592167454\t\n",
      "DAI62779 - ELE92920\t877\t0.0281984502106\t\n",
      "FRO40251 - FRO92469\t835\t0.026848011318\t\n",
      "DAI62779 - ELE32164\t832\t0.0267515513971\t\n",
      "DAI75645 - GRO73461\t712\t0.0228931545609\t\n",
      "DAI43223 - ELE32164\t711\t0.022861001254\t\n",
      "DAI62779 - GRO30386\t709\t0.02279669464\t\n",
      "ELE17451 - FRO40251\t697\t0.0224108549564\t\n",
      "DAI85309 - ELE99737\t659\t0.0211890292917\t\n",
      "DAI62779 - ELE26917\t650\t0.020899649529\t\n",
      "GRO21487 - GRO73461\t631\t0.0202887366966\t\n",
      "DAI62779 - SNA45677\t604\t0.0194205974084\t\n",
      "ELE17451 - SNA80324\t597\t0.0191955242597\t\n",
      "DAI62779 - GRO71621\t595\t0.0191312176457\t\n",
      "DAI62779 - SNA55762\t593\t0.0190669110318\t\n",
      "DAI62779 - DAI83733\t586\t0.018841837883\t\n",
      "ELE17451 - GRO73461\t580\t0.0186489180412\t\n",
      "GRO73461 - SNA80324\t562\t0.0180701585158\t\n",
      "DAI62779 - GRO59710\t561\t0.0180380052088\t\n",
      "DAI62779 - FRO80039\t550\t0.0176843188322\t\n",
      "DAI75645 - ELE17451\t547\t0.0175878589113\t\n",
      "DAI62779 - SNA93860\t537\t0.0172663258416\t\n",
      "DAI55148 - DAI62779\t526\t0.016912639465\t\n",
      "DAI43223 - GRO59710\t512\t0.0164624931674\t\n",
      "ELE17451 - ELE32164\t511\t0.0164303398605\t\n",
      "DAI62779 - SNA18336\t506\t0.0162695733256\t\n",
      "ELE32164 - GRO73461\t486\t0.0156265071863\t\n",
      "DAI62779 - FRO78087\t482\t0.0154978939584\t\n",
      "DAI85309 - ELE17451\t482\t0.0154978939584\t\n",
      "DAI62779 - GRO94758\t479\t0.0154014340375\t\n",
      "DAI62779 - GRO21487\t471\t0.0151442075817\t\n",
      "GRO85051 - SNA80324\t471\t0.0151442075817\t\n",
      "ELE17451 - GRO30386\t468\t0.0150477476608\t\n",
      "FRO85978 - SNA95666\t463\t0.014886981126\t\n",
      "DAI62779 - FRO19221\t462\t0.014854827819\t\n",
      "DAI62779 - GRO46854\t461\t0.0148226745121\t\n",
      "DAI43223 - DAI62779\t459\t0.0147583678981\t\n",
      "ELE92920 - SNA18336\t455\t0.0146297546703\t\n",
      "DAI88079 - FRO40251\t446\t0.0143403749076\t\n",
      "\n",
      "----- Least frequent but not less than 100 ------\n",
      "\n",
      "FRO40251 - GRO50832\t100\t0.00321533069676\t\n",
      "FRO40251 - GRO56989\t100\t0.00321533069676\t\n",
      "FRO78087 - GRO30386\t100\t0.00321533069676\t\n",
      "FRO78087 - GRO94758\t100\t0.00321533069676\t\n",
      "FRO80039 - GRO64900\t100\t0.00321533069676\t\n",
      "GRO38814 - SNA93860\t100\t0.00321533069676\t\n",
      "GRO46854 - SNA66583\t100\t0.00321533069676\t\n",
      "GRO59710 - SNA93860\t100\t0.00321533069676\t\n",
      "GRO64900 - SNA45677\t100\t0.00321533069676\t\n",
      "GRO73461 - GRO88511\t100\t0.00321533069676\t\n"
     ]
    }
   ],
   "source": [
    "print \"-------  Time to count and sort pairs  --------\\n\"\n",
    "print \"Count time:\\t %f sec\\nSort time:\\t %f sec\\nTotal time:\\t %f sec \" % (countTime,sortTime,countTime+sortTime)\n",
    "print \"\\n------------- Most frequent pairs --------------\\n\"\n",
    "print \"\\nPair                Support count     Support \\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.4/pairs_sorted/part-00000 | head -n 50\n",
    "print \"\\n----- Least frequent but not less than 100 ------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.4/pairs_sorted/part-00000 | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Report the compute time for the Pairs job:*     \n",
    "\n",
    "```\n",
    "Count time:   29.448766 sec\n",
    "Sort time:    21.381309 sec\n",
    "Total time:   50.830075 sec   \n",
    "```\n",
    "\n",
    "*Describe the computational setup used:*    \n",
    "   \n",
    "MacBook Pro, 2.5 GHz Intel Core i7  (4 Cores) \n",
    "16 GB 1600 MHz DDR3   \n",
    "\n",
    "Launched map tasks=2   \n",
    "Launched reduce tasks=1   \n",
    "\n",
    "![\"counters 3.4\"](http://candpgeneration.com/images/counters3.4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.5: Stripes\n",
    ">Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    ">Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    ">Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripesMapper_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripesMapper_1.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "############################################################\n",
    "# Using order inversion, we are able to count the total\n",
    "# number of baskets to send to the reducer for calculating\n",
    "# the relative frequencies.\n",
    "############################################################\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "N_BASKETS = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    N_BASKETS += 1\n",
    "    H = {}\n",
    "    for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "        if subset[0] not in H.keys():\n",
    "            H[subset[0]] = {}\n",
    "            H[subset[0]][subset[1]] = 1 \n",
    "        elif subset[1] not in H[subset[0]]:\n",
    "            H[subset[0]][subset[1]] = 1\n",
    "        else:\n",
    "            H[subset[0]][subset[1]] += 1 \n",
    "\n",
    "    for key in H.keys():\n",
    "        print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            \n",
    "            \n",
    "print \"%s\\t%s\" % (\"!N_BASKETS\", N_BASKETS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripesReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripesReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import collections as cl\n",
    "import json\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "N_BASKETS = 0\n",
    "cur_key = None\n",
    "cur_counter = cl.Counter()\n",
    "support_count = 100\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "        \n",
    "    key,value = line.split(\"\\t\")\n",
    "    if key == \"!N_BASKETS\":\n",
    "        N_BASKETS += int(value)\n",
    "        \n",
    "    elif key == cur_key:\n",
    "        cur_counter.update(json.loads(value))\n",
    "    else:\n",
    "        if cur_key:\n",
    "            for k in cur_counter.keys():\n",
    "                print '%s\\t%s\\t%s' % (cur_key+\" - \"+k, cur_counter[k], str(cur_counter[k]/N_BASKETS))\n",
    "        cur_key = key\n",
    "        cur_counter = cl.Counter(json.loads(value))\n",
    "\n",
    "for k in cur_counter.keys():\n",
    "    print '%s\\t%s\\t%s' % (cur_key+\" - \"+k, cur_counter[k], str(cur_counter[k]/N_BASKETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/01 08:32:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.4/stripes_sorted\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar5594139117926141974/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob5267287444497382553.jar tmpDir=null\n",
      "16/06/01 08:32:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/01 08:32:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/06/01 08:32:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/01 08:32:23 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/01 08:32:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464572970182_0138\n",
      "16/06/01 08:32:24 INFO impl.YarnClientImpl: Submitted application application_1464572970182_0138\n",
      "16/06/01 08:32:24 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1464572970182_0138/\n",
      "16/06/01 08:32:24 INFO mapreduce.Job: Running job: job_1464572970182_0138\n",
      "16/06/01 08:32:30 INFO mapreduce.Job: Job job_1464572970182_0138 running in uber mode : false\n",
      "16/06/01 08:32:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/01 08:32:39 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/01 08:32:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/01 08:32:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/01 08:32:47 INFO mapreduce.Job: Job job_1464572970182_0138 completed successfully\n",
      "16/06/01 08:32:47 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37713137\n",
      "\t\tFILE: Number of bytes written=75787146\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=35086162\n",
      "\t\tHDFS: Number of bytes written=35958941\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15082\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4745\n",
      "\t\tTotal time spent by all map tasks (ms)=15082\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4745\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15082\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4745\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15443968\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4858880\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=877095\n",
      "\t\tMap output records=877095\n",
      "\t\tMap output bytes=35958941\n",
      "\t\tMap output materialized bytes=37713143\n",
      "\t\tInput split bytes=220\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877095\n",
      "\t\tReduce shuffle bytes=37713143\n",
      "\t\tReduce input records=877095\n",
      "\t\tReduce output records=877095\n",
      "\t\tSpilled Records=1754190\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=322\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=599261184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=35085942\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35958941\n",
      "16/06/01 08:32:47 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.4/stripes_sorted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Count\n",
    "start = time.time()\n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.4/stripes\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D stream.num.map.output.key.field=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "    -files stripesMapper_1.py,stripesCombiner.py,stripesReducer.py \\\n",
    "    -mapper stripesMapper_1.py \\\n",
    "    -reducer stripesReducer.py \\\n",
    "    -input /user/koza/hw3/3.2.1/ProductPurchaseData.txt -output /user/koza/hw3/3.4/stripes\n",
    "end = time.time()\n",
    "countTime = end-start\n",
    "\n",
    "# Sort\n",
    "start = time.time()   \n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.4/stripes_sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D stream.num.map.output.key.field=2 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input /user/koza/hw3/3.4/stripes/* -output /user/koza/hw3/3.4/stripes_sorted    \n",
    "\n",
    "end = time.time()\n",
    "sortTime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  Time to count and sort stripes  --------\n",
      "\n",
      "Count time:\t 30.562926 sec\n",
      "Sort time:\t 27.477104 sec\n",
      "Total time:\t 58.040030 sec \n",
      "\n",
      "------------- Most frequent pairs --------------\n",
      "\n",
      "Pair                Support count     Support \n",
      "\n",
      "DAI62779 - ELE17451\t1592\t0.0511880646925\t\n",
      "FRO40251 - SNA80324\t1412\t0.0454004694383\t\n",
      "DAI75645 - FRO40251\t1254\t0.0403202469374\t\n",
      "FRO40251 - GRO85051\t1213\t0.0390019613517\t\n",
      "DAI62779 - GRO73461\t1139\t0.0366226166361\t\n",
      "DAI75645 - SNA80324\t1130\t0.0363332368734\t\n",
      "DAI62779 - FRO40251\t1070\t0.0344040384554\t\n",
      "DAI62779 - SNA80324\t923\t0.0296775023311\t\n",
      "DAI62779 - DAI85309\t918\t0.0295167357963\t\n",
      "ELE32164 - GRO59710\t911\t0.0292916626475\t\n",
      "DAI62779 - DAI75645\t882\t0.0283592167454\t\n",
      "FRO40251 - GRO73461\t882\t0.0283592167454\t\n",
      "DAI62779 - ELE92920\t877\t0.0281984502106\t\n",
      "FRO40251 - FRO92469\t835\t0.026848011318\t\n",
      "DAI62779 - ELE32164\t832\t0.0267515513971\t\n",
      "DAI75645 - GRO73461\t712\t0.0228931545609\t\n",
      "DAI43223 - ELE32164\t711\t0.022861001254\t\n",
      "DAI62779 - GRO30386\t709\t0.02279669464\t\n",
      "ELE17451 - FRO40251\t697\t0.0224108549564\t\n",
      "DAI85309 - ELE99737\t659\t0.0211890292917\t\n",
      "DAI62779 - ELE26917\t650\t0.020899649529\t\n",
      "GRO21487 - GRO73461\t631\t0.0202887366966\t\n",
      "DAI62779 - SNA45677\t604\t0.0194205974084\t\n",
      "ELE17451 - SNA80324\t597\t0.0191955242597\t\n",
      "DAI62779 - GRO71621\t595\t0.0191312176457\t\n",
      "DAI62779 - SNA55762\t593\t0.0190669110318\t\n",
      "DAI62779 - DAI83733\t586\t0.018841837883\t\n",
      "ELE17451 - GRO73461\t580\t0.0186489180412\t\n",
      "GRO73461 - SNA80324\t562\t0.0180701585158\t\n",
      "DAI62779 - GRO59710\t561\t0.0180380052088\t\n",
      "DAI62779 - FRO80039\t550\t0.0176843188322\t\n",
      "DAI75645 - ELE17451\t547\t0.0175878589113\t\n",
      "DAI62779 - SNA93860\t537\t0.0172663258416\t\n",
      "DAI55148 - DAI62779\t526\t0.016912639465\t\n",
      "DAI43223 - GRO59710\t512\t0.0164624931674\t\n",
      "ELE17451 - ELE32164\t511\t0.0164303398605\t\n",
      "DAI62779 - SNA18336\t506\t0.0162695733256\t\n",
      "ELE32164 - GRO73461\t486\t0.0156265071863\t\n",
      "DAI62779 - FRO78087\t482\t0.0154978939584\t\n",
      "DAI85309 - ELE17451\t482\t0.0154978939584\t\n",
      "DAI62779 - GRO94758\t479\t0.0154014340375\t\n",
      "DAI62779 - GRO21487\t471\t0.0151442075817\t\n",
      "GRO85051 - SNA80324\t471\t0.0151442075817\t\n",
      "ELE17451 - GRO30386\t468\t0.0150477476608\t\n",
      "FRO85978 - SNA95666\t463\t0.014886981126\t\n",
      "DAI62779 - FRO19221\t462\t0.014854827819\t\n",
      "DAI62779 - GRO46854\t461\t0.0148226745121\t\n",
      "DAI43223 - DAI62779\t459\t0.0147583678981\t\n",
      "ELE92920 - SNA18336\t455\t0.0146297546703\t\n",
      "DAI88079 - FRO40251\t446\t0.0143403749076\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "print \"-------  Time to count and sort stripes  --------\\n\"\n",
    "print \"Count time:\\t %f sec\\nSort time:\\t %f sec\\nTotal time:\\t %f sec \" % (countTime,sortTime,countTime+sortTime)\n",
    "print \"\\n------------- Most frequent pairs --------------\\n\"\n",
    "print \"Pair                Support count     Support \\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.4/stripes_sorted/part-00000 | head -n 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Report the compute time for the Stripes job:*     \n",
    "\n",
    "```\n",
    "Count time:   30.562926 sec\n",
    "Sort time:    27.477104 sec\n",
    "Total time:   58.040030 sec     \n",
    "```\n",
    "\n",
    "*Describe the computational setup used:*    \n",
    "   \n",
    "MacBook Pro, 2.5 GHz Intel Core i7  (4 Cores)    \n",
    "16 GB 1600 MHz DDR3   \n",
    "\n",
    "Launched map tasks=2   \n",
    "Launched reduce tasks=1   \n",
    "\n",
    "![\"counters 3.5\"](http://candpgeneration.com/images/counters3.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs vs Stripes comparison\n",
    "The pairs job finshed slightly faster than the stripes job. I expected the stripes job to perform better, since the opportunity to aggregate counts is higher with the stripes approach. On the other hand, values in the stripes approach are more complex, and come with more serialization and de-serialization overhead than with the pairs approach. \n",
    "\n",
    "It's important to consider these tradeoffs when thinking in terms of scalability. Stripes have the advantage of less network traffic, but one must be careful that each stripe can fit in memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
